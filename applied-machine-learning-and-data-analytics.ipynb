{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"**Machine Learning is the science of getting computers to learn and act like humans do, and improve their learning over time in autonomous fashion, by feeding them data and information in the form of observations and real-world interactions.\nThere are many algorithm for getting machines to learn, from using basic decision trees to clustering to layers of artificial neural networks depending on what task you’re trying to accomplish and the type and amount of data that you have available.  \n**"},{"metadata":{},"cell_type":"markdown","source":"**There are three types of machine learning** \n1. Supervised Machine Learning \n2. Unsupervised Machine Learning \n3. Reinforcement Machine Learning "},{"metadata":{},"cell_type":"markdown","source":"# Supervised Machine Learning\n **It is a type of learning in which both input and desired output data are provided. Input and output data are labeled for classification to provide a learning basis for future data processing.This algorithm consist of a target / outcome variable (or dependent variable) which is to be predicted from a given set of predictors (independent variables). Using these set of variables, we generate a function that map inputs to desired outputs. The training process continues until the model achieves a desired level of accuracy on the training data.**\n"},{"metadata":{},"cell_type":"markdown","source":"# Application of Supervised Machine Learning \n1. Bioinformatics \n2. Quantitative structure \n3. Database marketing \n4. Handwriting recognition \n5. Information retrieval \n6. Learning to rank \n7. Information extraction \n8. Object recognition in computer vision \n9. Optical character recognition \n10. Spam detection \n11. Pattern recognition \n\n"},{"metadata":{},"cell_type":"markdown","source":"# Application of Unsupervised Machine Learning \n1. Human Behaviour Analysis \n2. Social Network Analysis to define groups of friends. \n3. Market Segmentation of companies by location, industry, vertical. \n4. Organizing computing clusters based on similar event patterns and processes. \n"},{"metadata":{},"cell_type":"markdown","source":"# Application of Reinforcement Machine Learning \n1. Resources management in computer clusters \n2. Traffic Light Control \n3. Robotics \n4. Web System Configuration \n5. Personalized Recommendations \n6. Deep Learning \n"},{"metadata":{},"cell_type":"markdown","source":"# We can apply machine learning model by following six steps:-\n1. Problem Definition \n2. Analyse Data \n3. Prepare Data \n4. Evaluate Algorithm \n5. Improve Results \n6. Present Results \n"},{"metadata":{},"cell_type":"markdown","source":"# Factors help to choose algorithm \n1. Type of algorithm \n2. Parametrization \n3. Memory size \n4. Overfitting tendency \n5. Time of learning \n6. Time of predicting"},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression \n**It is a basic and commonly used type of predictive analysis. These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. \nY = a + bX where **\n* Y – Dependent Variable \n* a – intercept \n* X – Independent variable \n* b – Slope \n\n**Example: University GPA' = (0.675)(High School GPA) + 1.097**"},{"metadata":{},"cell_type":"markdown","source":"**Library and Data **"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\ntrain = pd.read_csv(\"../input/random-linear-regression/train.csv\") \ntest = pd.read_csv(\"../input/random-linear-regression/train.csv\") \ntrain = train.dropna()\ntest = test.dropna()\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model with plots and accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.array(train.iloc[:, :-1].values)\ny_train = np.array(train.iloc[:, 1].values)\nX_test = np.array(test.iloc[:, :-1].values)\ny_test = np.array(test.iloc[:, 1].values)\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\n\nplt.plot(X_train, model.predict(X_train), color='green')\nplt.show()\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression \n**It’s a classification algorithm, that is used where the response variable is categorical. The idea of Logistic Regression is to find a relationship between features and probability of particular outcome.**   \n* odds= p(x)/(1-p(x)) = probability of event occurrence / probability of not event occurrence \n\n**Example- When we have to predict if a student passes or fails in an exam when the number of hours spent studying is given as a feature, the response variable has two values, pass and fail. \n**"},{"metadata":{},"cell_type":"markdown","source":"**Libraries and data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import r2_score\nfrom statistics import mode\n\n\ntrain = pd.read_csv(\"../input/titanic/train.csv\")\ntest  = pd.read_csv('../input/titanic/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ports = pd.get_dummies(train.Embarked , prefix='Embarked')\ntrain = train.join(ports)\ntrain.drop(['Embarked'], axis=1, inplace=True)\ntrain.Sex = train.Sex.map({'male':0, 'female':1})\ny = train.Survived.copy()\nX = train.drop(['Survived'], axis=1) \nX.drop(['Cabin'], axis=1, inplace=True) \nX.drop(['Ticket'], axis=1, inplace=True) \nX.drop(['Name'], axis=1, inplace=True) \nX.drop(['PassengerId'], axis=1, inplace=True)\nX.Age.fillna(X.Age.median(), inplace=True) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model and Accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(max_iter = 500000)\nmodel.fit(X_train, y_train)\nmodel.score(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machine \n**Support Vector Machines are perhaps one of the most popular and talked about machine learning algorithms.It is primarily a classier method that performs classification tasks by constructing hyperplanes in a multidimensional space that separates cases of different class labels. SVM supports both regression and classification tasks and can handle multiple continuous and categorical variables \n**\n\n**Example: One class is linearly separable from the others like if we only had two features like Height and Hair length of an individual, we’d first plot these two variables in two dimensional space where each point has two co-ordinates **"},{"metadata":{},"cell_type":"markdown","source":"**Libraries and Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\ndata_svm = pd.read_csv(\"../input/svm-classification/UniversalBank.csv\")\ndata_svm.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model and Accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_svm.iloc[:,1:13].values\ny = data_svm.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\naccuracies.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes Algorithm \n**A naive Bayes classifier is not a single algorithm, but a family of machine learning algorithms which use probability theory to classify data with an assumption of independence between predictors It is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods    \n**\n\n**Example: Emails are given and we have to find the spam emails from that.A spam filter looks at email messages for certain key words and puts them in a spam folder if they match.**"},{"metadata":{},"cell_type":"markdown","source":"**Libraries and Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\ndata = pd.read_csv('../input/classification-suv-dataset/Social_Network_Ads.csv')\ndata_nb = data\ndata_nb.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model and Accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_nb.iloc[:, [2,3]].values\ny = data_nb.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=GaussianNB()\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN \n**KNN does not learn any model. and stores the entire training data set which it uses as its representation.The output can be calculated as the class with the highest frequency from the K-most similar instances. Each instance in essence votes for their class and the class with the most votes is taken as the prediction \n**\n\n**Example: Should the bank give a loan to an individual? Would an individual default on his or her loan? Is that person closer in characteristics to people who defaulted or did not default on their loans? **\n"},{"metadata":{},"cell_type":"markdown","source":"**Libraries and Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = data\nknn.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model and Accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = knn.iloc[:, [2,3]].values\ny = knn.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest \n**Random forest is collection of tress(forest) and it builds multiple decision trees and merges them together to get a more accurate and stable prediction.It can be used for both classification and regression problems.**\n\n**Example: Suppose we have a bowl of 100 unique numbers from 0 to 99. We want to select a random sample of numbers from the bowl. If we put the number back in the bowl, it may be selected more than once. \n**"},{"metadata":{},"cell_type":"markdown","source":"**Libraries and Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = data\nrf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model and Accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = rf.iloc[:, [2,3]].values\ny = rf.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=RandomForestClassifier(n_estimators=1000,criterion='entropy',random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree\n**Decision tree algorithm is classification algorithm under supervised machine learning and it is simple to understand and use in data.The idea of Decision tree is to split the big data(root) into smaller(leaves)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = data\ndt.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dt.iloc[:, [2,3]].values\ny = dt.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=DecisionTreeClassifier(criterion=\"entropy\",random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting\n**Gradient boosting is an alogithm under supervised machine learning, boosting means converting weak into strong. In this new tree is boosted over the previous tree**"},{"metadata":{},"cell_type":"markdown","source":"**Libraries and Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = data\ngb.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model and Accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = gb.iloc[:, [2,3]].values\ny = gb.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\npred = gbk.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Light GBM"},{"metadata":{},"cell_type":"markdown","source":"**LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:**\n\n1. Faster training speed and higher efficiency.\n2. Lower memory usage.\n3. Better accuracy.\n4. Support of parallel and GPU learning.\n5. Capable of handling large-scale data."},{"metadata":{},"cell_type":"markdown","source":"**Library and Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgbm\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn import preprocessing\n\n\ntrain = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ndata = pd.concat([train, test], sort=False)\ndata = data.reset_index(drop=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing**"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"nans=pd.isnull(data).sum()\n\ndata['MSZoning']  = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\ndata['Utilities'] = data['Utilities'].fillna(data['Utilities'].mode()[0])\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\n\ndata[\"BsmtFinSF1\"]  = data[\"BsmtFinSF1\"].fillna(0)\ndata[\"BsmtFinSF2\"]  = data[\"BsmtFinSF2\"].fillna(0)\ndata[\"BsmtUnfSF\"]   = data[\"BsmtUnfSF\"].fillna(0)\ndata[\"TotalBsmtSF\"] = data[\"TotalBsmtSF\"].fillna(0)\ndata[\"BsmtFullBath\"] = data[\"BsmtFullBath\"].fillna(0)\ndata[\"BsmtHalfBath\"] = data[\"BsmtHalfBath\"].fillna(0)\ndata[\"BsmtQual\"] = data[\"BsmtQual\"].fillna(\"None\")\ndata[\"BsmtCond\"] = data[\"BsmtCond\"].fillna(\"None\")\ndata[\"BsmtExposure\"] = data[\"BsmtExposure\"].fillna(\"None\")\ndata[\"BsmtFinType1\"] = data[\"BsmtFinType1\"].fillna(\"None\")\ndata[\"BsmtFinType2\"] = data[\"BsmtFinType2\"].fillna(\"None\")\n\ndata['KitchenQual']  = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])\ndata[\"Functional\"]   = data[\"Functional\"].fillna(\"Typ\")\ndata[\"FireplaceQu\"]  = data[\"FireplaceQu\"].fillna(\"None\")\n\ndata[\"GarageType\"]   = data[\"GarageType\"].fillna(\"None\")\ndata[\"GarageYrBlt\"]  = data[\"GarageYrBlt\"].fillna(0)\ndata[\"GarageFinish\"] = data[\"GarageFinish\"].fillna(\"None\")\ndata[\"GarageCars\"] = data[\"GarageCars\"].fillna(0)\ndata[\"GarageArea\"] = data[\"GarageArea\"].fillna(0)\ndata[\"GarageQual\"] = data[\"GarageQual\"].fillna(\"None\")\ndata[\"GarageCond\"] = data[\"GarageCond\"].fillna(\"None\")\n\ndata[\"PoolQC\"] = data[\"PoolQC\"].fillna(\"None\")\ndata[\"Fence\"]  = data[\"Fence\"].fillna(\"None\")\ndata[\"MiscFeature\"] = data[\"MiscFeature\"].fillna(\"None\")\ndata['SaleType']    = data['SaleType'].fillna(data['SaleType'].mode()[0])\ndata['LotFrontage'].interpolate(method='linear',inplace=True)\ndata[\"Electrical\"]  = data.groupby(\"YearBuilt\")['Electrical'].transform(lambda x: x.fillna(x.mode()[0]))\ndata[\"Alley\"] = data[\"Alley\"].fillna(\"None\")\n\ndata[\"MasVnrType\"] = data[\"MasVnrType\"].fillna(\"None\")\ndata[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(0)\nnans=pd.isnull(data).sum()\nnans[nans>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_list = []\nfor col in data.columns:\n    if type(data[col][0]) == type('str'): \n        _list.append(col)\n\nle = preprocessing.LabelEncoder()\nfor li in _list:\n    le.fit(list(set(data[li])))\n    data[li] = le.transform(data[li])\n\ntrain, test = data[:len(train)], data[len(train):]\n\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\n\ntest = test.drop(columns=['SalePrice', 'Id'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model and Accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = KFold(n_splits=5, random_state = 2020, shuffle = True)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nmodel_lgb.fit(X, y)\nr2_score(model_lgb.predict(X), y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **LDA**"},{"metadata":{},"cell_type":"markdown","source":"**A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.Itis  used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.**"},{"metadata":{},"cell_type":"markdown","source":"**Library and Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = data\nlda.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model and Accuracy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = gb.iloc[:, [2,3]].values\ny = gb.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nModel=LinearDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint('accuracy is ',accuracy_score(y_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Means Algorithm \nK-means clustering is a type of unsupervised learning, which is used when you have unlabeled data and the goal of this algorithm is to find groups in the data \n\n**Steps to use this algorithm:-**\n* 1-Clusters the data into k groups where k is predefined. \n* 2-Select k points at random as cluster centers. \n* 3-Assign objects to their closest cluster center according to the Euclidean distance function. \n* 4-Calculate the centroid or mean of all objects in each cluster. \n\n**Examples: Behavioral segmentation like segment by purchase history or by activities on application, website, or platform Separate valid activity groups from bots  **\n"},{"metadata":{},"cell_type":"markdown","source":"**Libraries and Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nkm = pd.read_csv(\"../input/k-mean/km.csv\")\nkm.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking for number of clusters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"K_clusters = range(1,8)\nkmeans = [KMeans(n_clusters=i) for i in K_clusters]\nY_axis = km[['latitude']]\nX_axis = km[['longitude']]\nscore = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]\nplt.plot(K_clusters, score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fitting Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 3, init ='k-means++')\nkmeans.fit(km[km.columns[1:3]])\nkm['cluster_label'] = kmeans.fit_predict(km[km.columns[1:3]])\ncenters = kmeans.cluster_centers_\nlabels = kmeans.predict(km[km.columns[1:3]])\nkm.cluster_label.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting Clusters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"km.plot.scatter(x = 'latitude', y = 'longitude', c=labels, s=50, cmap='viridis')\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=100, alpha=0.5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CNN"},{"metadata":{},"cell_type":"markdown","source":"**Library and Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nimport tensorflow as tf\ntrain_data = pd.read_csv(\"../input/digit-recognizer/train.csv\")\ntest_data = pd.read_csv(\"../input/digit-recognizer/test.csv\")\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing and Data Split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(train_data.drop(\"label\", axis=1)).astype('float32')\ny = np.array(train_data['label']).astype('float32')\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(X[i].reshape(28, 28), cmap=plt.cm.binary)\n    plt.xlabel(y[i])\nplt.show()\n\nX = X / 255.0\nX = X.reshape(-1, 28, 28, 1)\ny = to_categorical(y)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\nX_test = np.array(test_data).astype('float32')\nX_test = X_test / 255.0\nX_test = X_test.reshape(-1, 28, 28, 1)\nplt.figure(figsize=(10,10))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Compiling model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#increse to epochs to 30 for better accuracy\nmodel.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=85, validation_data=(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.show()\n\nprint(model.evaluate(X_val, y_val))\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"prediction = model.predict_classes(X_test)\nsubmit = pd.DataFrame(prediction,columns=[\"Label\"])\nsubmit[\"ImageId\"] = pd.Series(range(1,(len(prediction)+1)))\nsubmission = submit[[\"ImageId\",\"Label\"]]\nsubmission.to_csv(\"submission.csv\",index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prophet"},{"metadata":{},"cell_type":"markdown","source":"\nProphet is an extremely easy tool for analysts to produce reliable forecasts"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"1. Prophet only takes data as a dataframe with a ds (datestamp) and y (value we want to forecast) column. So first, let’s convert the dataframe to the appropriate format.\n1. Create an instance of the Prophet class and then fit our dataframe to it.\n2. Create a dataframe with the dates for which we want a prediction to be made with make_future_dataframe(). Then specify the number of days to forecast using the periods parameter.\n3. Call predict to make a prediction and store it in the forecast dataframe. What’s neat here is that you can inspect the dataframe and see the predictions as well as the lower and upper boundaries of the uncertainty interval.\n"},{"metadata":{},"cell_type":"markdown","source":"**Library and Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.offline as py\nimport plotly.express as px\nfrom fbprophet import Prophet\nfrom fbprophet.plot import plot_plotly, add_changepoints_to_plot\n\npred = pd.read_csv(\"../input/coronavirus-2019ncov/covid-19-all.csv\")\npred = pred.fillna(0)\npredgrp = pred.groupby(\"Date\")[[\"Confirmed\",\"Recovered\",\"Deaths\"]].sum().reset_index()\npred_cnfrm = predgrp.loc[:,[\"Date\",\"Confirmed\"]]\npr_data = pred_cnfrm\npr_data.columns = ['ds','y']\npr_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model and Forecast**"},{"metadata":{"trusted":true},"cell_type":"code","source":"m=Prophet()\nm.fit(pr_data)\nfuture=m.make_future_dataframe(periods=15)\nforecast=m.predict(future)\nforecast\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plot_plotly(m, forecast)\npy.iplot(fig) \n\nfig = m.plot(forecast,xlabel='Date',ylabel='Confirmed Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Arima"},{"metadata":{},"cell_type":"markdown","source":"**Library and Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nfrom statsmodels.tsa.arima_model import ARIMA\nar = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\nar.date=ar.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))\nar=ar.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nar.index=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nar=ar.reset_index()\nar=ar.loc[:,[\"index\",\"item_cnt_day\"]]\nar.columns = ['confirmed_date','count']\nar.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(ar['count'].values, order=(1, 2, 1))\nfit_model = model.fit(trend='c', full_output=True, disp=True)\nfit_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prediction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_model.plot_predict()\nplt.title('Forecast vs Actual')\npd.DataFrame(fit_model.resid).plot()\nforcast = fit_model.forecast(steps=6)\npred_y = forcast[0].tolist()\npred = pd.DataFrame(pred_y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Evaluate Algorithms** \n**The evaluation of algorithm consist three following steps:- **\n1. Test Harness  \n2. Explore and select algorithms \n3. Interpret and report results \n\n"},{"metadata":{},"cell_type":"markdown","source":"# Data Analytics\n"},{"metadata":{},"cell_type":"markdown","source":"We will do detailed analysis on campus placement dataset "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cp = pd.read_csv(\"../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv\")\ncp = cp.fillna(0)\ncp.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bar Plot"},{"metadata":{},"cell_type":"markdown","source":"**Plotting bar graph of gender vs salary where specialisation is feature**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\ngrgs = cp.groupby([\"gender\",\"specialisation\"])[[\"salary\"]].mean().reset_index()\nfig = px.bar(grgs[['gender', 'salary','specialisation']].sort_values('salary', ascending=False), \n             y=\"salary\", x=\"gender\", color='specialisation', \n             log_y=True, template='ggplot2')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pie Plot"},{"metadata":{},"cell_type":"markdown","source":"**Plotting Pie Chart of Degree and percentage is a feature**"},{"metadata":{"trusted":true},"cell_type":"code","source":"grdsp = cp.groupby([\"degree_t\"])[[\"degree_p\"]].mean().reset_index()\n\nfig = px.pie(grdsp,\n             values=\"degree_p\",\n             names=\"degree_t\",\n             template=\"seaborn\")\nfig.update_traces(rotation=90, pull=0.05, textinfo=\"percent+label\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tree Plot"},{"metadata":{},"cell_type":"markdown","source":"**Plotting tree Chart of high secondary stream and percentage is a feature**"},{"metadata":{"trusted":true},"cell_type":"code","source":"grss = cp.groupby([\"hsc_s\"])[[\"hsc_p\"]].mean().reset_index()\n\nfig = px.treemap(grss, path=['hsc_s'], values='hsc_p',\n                  color='hsc_p', hover_data=['hsc_s'],\n                  color_continuous_scale='rainbow')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scatter Plot"},{"metadata":{},"cell_type":"markdown","source":"**Scatter Plot show the degree and salary**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(cp.degree_t,cp.salary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Line Chart"},{"metadata":{},"cell_type":"markdown","source":"Line chart show the degree and percentage in secondary, senior secondary and degree"},{"metadata":{"trusted":true},"cell_type":"code","source":"grd = cp.groupby([\"hsc_s\"])[[\"hsc_p\",\"ssc_p\",\"degree_p\"]].mean().reset_index()\nf, ax = plt.subplots(figsize=(100, 30))\n\nplt.plot(grd.hsc_s,grd.hsc_p,color=\"blue\")\nplt.plot(grd.hsc_s,grd.ssc_p,color=\"black\")\nplt.plot(grd.hsc_s,grd.degree_p,color=\"red\")\nplt.xticks(fontsize=50)\nplt.yticks(fontsize=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Density Plot**"},{"metadata":{},"cell_type":"markdown","source":"**The density plot show the distribution of salary**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(10,6))\nsns.set_style(\"darkgrid\")\nsns.kdeplot(data=cp['salary'],label=\"Salary\" ,shade=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install chart_studio\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"pip install bubbly\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Bubble Plot**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from bubbly.bubbly import bubbleplot \nfrom plotly.offline import iplot\nimport chart_studio.plotly as py\nm = pd.read_csv(\"../input/global-hospital-beds-capacity-for-covid19/hospital_beds_USA_v1.csv\")\n\n\nfigure = bubbleplot(dataset=m, x_column='beds', y_column='population', \n    bubble_column='state', size_column='beds', color_column='type', \n    x_logscale=True, scale_bubble=2, height=350)\n\niplot(figure)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heat Map"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nf, ax = plt.subplots(figsize=(15,2))\nh=pd.pivot_table(cp,columns='sl_no',values=[\"salary\"])\nsns.heatmap(h,cmap=['skyblue','red','green'],linewidths=0.05)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# **Folium Map**"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = pd.read_csv(\"../input/global-hospital-beds-capacity-for-covid19/hospital_beds_USA_v1.csv\")\n\nimport folium\nmap = folium.Map(location=[37.0902,-95.7129 ], zoom_start=4,tiles='cartodbpositron')\n\nfor lat, lon,state,type in zip(m['lat'], m['lng'],m['state'],m['type']):\n    folium.CircleMarker([lat, lon],\n                        radius=5,\n                        color='red',\n                      popup =(\n                    'State: ' + str(state) + '<br>'),\n\n                        fill_color='red',\n                        fill_opacity=0.7 ).add_to(map)\nmap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Choropleth"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.choropleth(m, locations=m[\"state\"],       \n\n color=m[\"beds\"],\n                    locationmode=\"USA-states\",\n                    scope=\"usa\",\n                    color_continuous_scale='Reds',\n                   )\n\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}